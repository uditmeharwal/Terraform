it is an orchestration tool . it is also called k8s
monolyth - it is the application in which all the application included in a single application , it is very hard to manage , it is a single repository 
micro services -it is the collection of different component of an application which are devided according to their work , and incase any service may not working so we can maanage this without stoping the app 
for managing docker relaeted service we use kubernetes 
In this we can handel many coontainers or we can say we are working on a cluster 
--------------------------------------------------------------------------
--Architecture of kubernetes--
scheduler - it manage all time related activites and manage all schedules 
api server -it help to  communicate with worker leve 
controller manager - it is the head manager and manage all the activities 
pods -pods is the place where containers are running 
etcd - it is the database of the kubernetes
kublet - low level manager , it work as worker node 
service proxy 
kubectl - help to control and instruct to hole cluster 
------------------------------------------------------------------------
--- kubernetes cluster types -----
-> kubeadm - we use two or more instance and attach them together and make a cluster , it is very expensive  
-> minikube (local system )
-> kind cluster -> kubernetes in docker 
-> EKS /AKS -> Elastic kubernetes service it is online cloud service 
--------------------------------------------------------------------------------------------------------------
--------- kind cluster---- install and run  
-> create an instance in aws and ssh it in system 
-> open kubestarter repositary in git hub and open kind cluster 
-> create a file install.sh and paste the command in it and give them full prmission and  run it in system 
-> install docker in your system 
-> kubectl version -> show version 
-> kind --version -> show version 
-> now create a kubernetes cluster 
-> check second step in git repo and create a config.yml file and paste the code for cluster in it 
->
                kind: Cluster
                apiVersion: kind.x-k8s.io/v1alpha4

                nodes:
                  - role: control-plane
                    image: kindest/node:v1.31.2
                  - role: worker
                    image: kindest/node:v1.31.2
                  - role: worker
                    image: kindest/node:v1.31.2

-> kind create cluster --config kind-cluster-config.yaml --name my-kind-cluster
-> kubectl get nodes -> information about all nodes 
-----------------------------------------------------------------------------------------------------------------
------- mini kube --------- install and run 
-> open git and shubham lodhe repo and open kubestarter 
-> open mini kube installer file and follow steps 
-> kubectl config user-context udit-cluster  ---> restore to the normal settings 
-----------------------------------------------------------------------------------------------------------------
------- kubeadm ----- install and run 
- create 2 instance  and ssh them in systwm 
- open kubeadm in git and follow commands
-----------------------------------------------------------------------------------------------------------------
- POD -
one pod can run one or more containers 
- namespace - the particular resources present in the namespace are belong to the particular group 
- docker container -> pod -> deployment -> service -> user access
- kubectl get ns -> show all name spaces 
- kubectl get pods -> show podes 
- kubectl create ns ngnix - create namespance ngenix 
- kubectl run nginix --image=nginix -> it will create a pod of nginix
- kubectl get pods - show list of pods 
- kubectl delete pod nginix - delete the pods 
- kubectl run nginix --image=nginix -n nginix - create th pod in particular name space 
- kubectl get pods -n nginix - list of the pods in the nginix name space 
-kubectl get pods -n nginix -o wide - give more info about pods 
- we can create all the pods ,deloyment , service through yml file which is called manifest file 
      -create a file namespace.yml
          -kind: Namespace
           apiVersion: v1 
           metadata: 
              name: nginix
      -save the file and use command "kubectl apply -f namespace.yml"
-now we create a pod 
    --create a file pod.yml 
        kind: pod 
        apiVersion: v1 
        metadata: 
          name: nginix-pod 
          namespace: nginix 
        spec: 
          containers: 
          - name: nginix
            image: nginix:latest
            ports: 
            - containerPost: 80
      - again use command "kubectl apply -f pod.yml"
-kubectl exec -it nginix-pod -n nginix -- bash     - with this we can enter inside the pods 

-Replica Set --> it create multiple copy of a pod
-Stateful Set -> it give address to a pod so that their state will maintain
-Deployment Set -> it  create replicasa and also give facility of rolling updates
              Rolling Updates - it allow to run the application during updates and prevent the app to stop . it give the update to podes one by one 
-Daemon Set ->  it ensure that each nodes have assigne atlest on pod running on it  

--DEPLOYMENT--  

- create a file deployment.yml and enter code in in 
      
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginix-dep-pod 
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
          
-kubecut apply -f deployment.yml - apply the file 
- kubectl scale deployment/nginix-deployment  -n nginix --replicas=5  - incase of heavy traffic we can increase the pod 
- kubectl set image deployment/nginix-deployment -n nginix niginx=nginx:1.27.3 = command is use to roll over new update of nginix in the pods 

--Replica Set-- we can only replicate the podes not use other features 

- create file Replicasets.yml
    kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: nginx-replicasets
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginix-rep-pod 
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

- now apply 

-- Daemon Sets --

- create a file Daemonaset.yml

          kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: nginx-daemonsets
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginix-dem-pod 
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

--now apply this 


-- jobs and crown jobs --

- job - the task done by single container
 - create job.yml
      
kind: Job
apiVersion: batch/v1
metadata:
  name: demo-job
  namespace: nginix
spec:
complition: 1
parallelism: 1
  template:
    metadata:
      name: demo-job-pod
      labels:
        app: batch-task
    spec:
      containers:
      - name: batch-container
        image : busybox: latest
        command: ["sh", "-c" ,"echo hello ! && sleep 10"]
        restartPolicy: never

- now apply 
- kubectl get job -n nginix  - check job status 
- kubectl logs pod/demo-job -n nginix - give out tht log 

-cron job - if the particular task run on any specific time . following a particular schedula and run the command 

-create cronJob.yml
      
kind: CronJob
apiVersion: batch/v1
metadata:
  name: minutUpdate e-backup
  namespace: nginix
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          name: hello
            labels:
              app: hello
          spec:
            containers:
            - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure

- cron guru - it is site use for scheduling the time 
- now apply

-------------------------------------------------------------------------------------
--STORAGE--

--persistant volume-- 

-create persistant volume.yml
        kind: PersistentVolume
        apiVersion: v1
        metadata:
            name: local-pv
            labels:
              app: local
        spec:
            capacity
              storage: 1Gi
            accessModes:
              -ReadWriteOnce
            persistentVolumeRFeclaimPolicy:Retain
            storageClassName: local-storage
            hostPath:
                path: /mnt/data
- now apply command
 -kubectl get pv - show the list of persistant volume 
after create the volume we need to attach it to local system 
-vim persestent volumeclaim.yml -help to claim the volume 


  kind: PersistentVolumeClaim
  apiVersions: v1
  metadata: 
    name: local-pvc
  spec: 
    accessModes:
      -ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  StorageClassName: local-storage 

- apple this in system
-kubectl get pvc -show list of pvc 

now we use this volume in our app by applying volume mouunt command in 

open deployment.yml 

kind: Deployment
apiVersion: apps/v1
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginix-dep-pod 
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
          volumeMounts:
            - mountPath : /var/www/html
              name: local-pvc
          volumes: 
            - name: my-volume
              persistentVolumeClaim:
                  claimName: local-pvc
- now apply 
- open persistent volume .yml  and add namespace : nginix 
- open persistent volume claim.yml and namespace : ngiix
- now again apply both  

--------------------------------------------------------------------------------    
-- Networking --
Service - it is use to access the pods information to the public user 
- create service.yml
    
kind: Service
apiVersion: v1
metadata:
  name: nginix-service
  namespace: nginix
spec:
  selector:
    app: nginix
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

-- now apply 
- kubectl port-forward service/nginix-service -n nginix 80:80  --address=0.0.0.0 - this command will allow us to access the nginix publically 

---Ingress---
it is traffic rerouting tool in which we can create routes for the links link /movie /game divert to different links 

-ingress controller - help in routing in cluster level 
-- open kind documentation and open ingress section and follow 
-kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/deploy-ingress-nginx.yaml
-create ingress.yml 
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata: 
        name: nginix-notes-ingress
        namespace: nginix 
        annotations:
            nginix.ingress.kubernetes.io/rewrite-target: /
      spec:
        rules:
          - http: 
              paths: 
                -pathType: prefix
                  path: /nginix
                    backend: 
                      service:
                        name: nginix-service
                        port: 80 
                          number: 0
                  - pathType: Prefix
                    path: /
                    backend:
                      service:
                          name:notes-app-service
                          port: 
                            number:8000
  - now apply 
- kubectl port-forward service/ingress-nginix-controller -n ingress-nginix 8080:80 --address=0.0.0.0 for ingress controller  
                
.............................................................
--- django notes app mini project --
- copy git django  ote app 
- create image using docker
- login in docker hub 
- create tocken in docker hub for login 
- login in system 
- tag the image 
- push the image in docker hub 
- now create namespace.yml
- now create deployment .yml
- now create service .yml 
- first apply namespace , then deployment , then service 
- now expose the app to the public 
- now the app is online 
.............................................................
 -- Stateful sets --
- mainly use in databases
- firstly we create a new namespace called mysql
  create file namespace.yml 
      kind: NamesSpace        
      apiVersion v1
      metadata:
          name: mysql 
-now apply it 

- now create statefulsets.yml 
      kind: StatefulSet
      apiVersion: apps/v1
      metadata: 
        name: mysql-statefulset
        namespace: mysql
      spec:
        serviceName: mysql-service
        replicas: 3
        selector:
            matchLabels:  
                app: mysql
     template:
            metadata:
              labels:
                app: mysql
        spec:
          containers:
          - name: mysql
            image: sql:8.0
            ports:
            - containerPort: 3306    
            env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom: 
                SecretKeyRef:
                    name: mysql-secret
                    key: MYSQL_ROOT_PASSWORD
            - name: MYSQL_DATABASE
              value: devops
           VolumeMount:
            -name: mysql-data
            mountpath: /var/lib/mysql
          VolumeClaimTemplates:
          - metadata:
              name: mysql-data
             spec:
              accessMode: [ReadWriteOnce]
              resources:
                requests:
                    storage: 1Gi

- now create service file for stateful create service.yml
        kind: Service
        apiVersion: v1
        metadata:
            name: mysql-service
            namespace: mysql
        spec:
          clusterIP: none 
          selector:
            app: mysql
          ports:
          - name: mysql
            protocpol: TCP
            port: 3306
            targetPort: 3306

- now apply service
-headless service - which cluster ip is none  means outer public cannot access it 
- kubectl exec -it mysql-statefulset-0 -n mysql --bash - we can enter instde mysql pod 
- kubectl delete pod mysql-statefulset-0 -n mysql   -delete the pod my sql 

-- configMaps -- 
it is the file which is use to store the data related to the configurations of all the things 
       create a file  configMap.yml
          kind: ConfigMap
          apiVersion: v1 
          metadata: 
              name: mysql-cofig-map
              namespace: mysql
          data: 
            MYSQL_DATABASE: devops 
-- now apply this 

now open statefulfile and change  the keyword value to this given below 

    use valueFrom: 
            configMapkeyRef:
            name: mysql-config-map
            key: MYSQL_DATABASE
            
-- secrets --
if we want to hide passwords and encript the passwords so we use secret .yml file 
- create a file secret.yml
        apiVersion: v1
        kind: Secret
        metadata: 
            name: mysql-secret
            namespace: mysql 
          data:
            MYSQL_ROOT_PASSWORD: root 
--- we can not directly enter the password here we need to convert it into base64 for the we use 
- echo "your_password" | base64
this will give an output and we use that output and paste it in password place in secret file 

-- now we need to define secret file in our stateful file like of config Map file  and then apply secret file 
 
-----------------------------------------------------------------------------------------------------------------
 --- Scaling and scheduling ---

-- Resource quotas and limits - resource allocation (ram , memory to any specific pod ) 
    it is written in deployment.yml 
          in the containers section 
            resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi

- now apply deployment 

----Probes --
types
- liveness probe - it will check that our probe is live or not 
      - this will apply on deployment file under the containerPort section 
          livenessProbe:
              httpGet:
              path: /
              port: 8000

- readiness probe - check tat it is ready or not 
            readinessProbe:
                httpGet:
                path: /
                port: 8000
- startup probe -

----Taints/Tolerations---
taint is the way to telling cluster that you are not allow to scheduled a pod on particular worker node 
toleration - if any node is tainted it allow us to scheduled the pod on particular node 
- kubectl taint node node-name prod=true:NoSchedule this connand use for taint the pod and cancel all the scheduling for the pod 
- kubectl taint node node-name prod=true:NoSchedule- for removing all taint we use minus at end 

how to use tainted node 

open pod.yml and enter after containeraPort
        tolerations:
        - key : "prod"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
- now apply pod 


Autoscaling and autohealing is one of the important work of the kubernetes and HPA and VPA ara part of this 

AutoScaling- as load increase the pods capasity and nuber will also increase 
-HPA - horizontal pod auto scaling 
 in this if the load increase the replicas of the pod create and number of pods increase 



 

-VPA - vertial pod auto scaling
in this if the load increase then the pod increase their resources and capacity they do not replicate 
mainly use in stateful application 

-KEDA - kubernetes event driven auto scaling 
it is very lessly using service in industry 
keda use both hpa and vpa on the basis of type of application and matrics of the application 

matrics - quantifiable resources number .inform about resource use 
- minikube addons enable metrics -server  this comand is for minikube to use metrics 
- for apply in kind cluster we use 





